"""
Malware Hash Checker Tool

This tool checks file hashes against multiple threat intelligence sources
and malware databases to identify potential threats.
"""

import re
import hashlib
import random
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Any, Optional, Tuple

try:
    from .schemas import (
        MalwareHashInput, MalwareHashOutput, ThreatIntelligenceResult,
        MalwareMetadata, ReputationAnalysis, HashValidation
    )
except ImportError:
    from schemas import (
        MalwareHashInput, MalwareHashOutput, ThreatIntelligenceResult,
        MalwareMetadata, ReputationAnalysis, HashValidation
    )


class MalwareHashChecker:
    """Malware hash checker with multiple threat intelligence sources"""
    
    # Hash type patterns
    HASH_PATTERNS = {
        'md5': r'^[a-fA-F0-9]{32}$',
        'sha1': r'^[a-fA-F0-9]{40}$',
        'sha256': r'^[a-fA-F0-9]{64}$',
        'sha512': r'^[a-fA-F0-9]{128}$'
    }
    
    # Known malicious hash samples (for demonstration)
    KNOWN_MALICIOUS_HASHES = {
        # WannaCry samples
        '84c82835a5d21bbcf75a61706d8ab549': 'wannacry',
        'ed01ebfbc9eb5bbea545af4d01bf5f1071661840480439c6e5babe8e080e41aa': 'wannacry',
        
        # Conficker samples  
        'b7c1e8c4f7b8e5c8e1f7b8e5c8e1f7b8': 'conficker',
        
        # Zeus samples
        '5f4c4f7b8e5c8e1f7b8e5c8e1f7b8e5c': 'zeus',
        
        # Emotet samples
        'a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6': 'emotet'
    }
    
    # Threat intelligence sources (simulated)
    THREAT_SOURCES = [
        'VirusTotal', 'Hybrid Analysis', 'Malware Bazaar', 'ThreatFox',
        'URLVoid', 'AlienVault OTX', 'IBM X-Force', 'Cisco Talos'
    ]
    
    # Malware families and their characteristics
    MALWARE_FAMILIES = {
        'wannacry': {
            'type': 'ransomware',
            'severity': 'critical',
            'description': 'WannaCry ransomware that exploits SMB vulnerabilities'
        },
        'conficker': {
            'type': 'worm',
            'severity': 'high',
            'description': 'Conficker worm that spreads via network shares'
        },
        'zeus': {
            'type': 'banking_trojan',
            'severity': 'high', 
            'description': 'Zeus banking trojan for credential theft'
        },
        'emotet': {
            'type': 'trojan',
            'severity': 'high',
            'description': 'Emotet trojan and malware dropper'
        }
    }
    
    def __init__(self):
        pass
    
    def validate_hash(self, hash_value: str) -> HashValidation:
        """Validate hash format and detect hash type"""
        
        hash_value = hash_value.strip().lower()
        hash_length = len(hash_value)
        issues = []
        detected_type = "unknown"
        is_valid = False
        
        # Check for valid hexadecimal characters
        if not re.match(r'^[a-fA-F0-9]+$', hash_value):
            issues.append("Hash contains invalid characters (not hexadecimal)")
        
        # Detect hash type based on length
        for hash_type, pattern in self.HASH_PATTERNS.items():
            if re.match(pattern, hash_value):
                detected_type = hash_type
                is_valid = True
                break
        
        if not is_valid:
            issues.append(f"Invalid hash length: {hash_length} characters")
            if hash_length not in [32, 40, 64, 128]:
                issues.append("Hash length doesn't match common algorithms (MD5, SHA1, SHA256, SHA512)")
        
        return HashValidation(
            is_valid_hash=is_valid,
            detected_hash_type=detected_type,
            hash_length=hash_length,
            hash_format_issues=issues
        )
    
    def check_known_malicious(self, hash_value: str) -> Optional[str]:
        """Check if hash is in known malicious samples"""
        return self.KNOWN_MALICIOUS_HASHES.get(hash_value.lower())
    
    def simulate_threat_intelligence_check(self, hash_value: str, source: str) -> ThreatIntelligenceResult:
        """Simulate checking against threat intelligence source"""
        
        # Check if hash is known malicious
        malware_family = self.check_known_malicious(hash_value)
        is_malicious = malware_family is not None
        
        # Add some randomness for simulation
        if not is_malicious:
            # Small chance of random detection for demonstration
            is_malicious = random.random() < 0.05  # 5% chance
        
        # Generate simulated data
        if is_malicious:
            detection_ratio = f"{random.randint(15, 65)}/{random.randint(65, 75)}"
            confidence_score = random.uniform(75, 95)
            threat_type = self.MALWARE_FAMILIES.get(malware_family, {}).get('type', 'malware')
        else:
            detection_ratio = f"0/{random.randint(65, 75)}" if random.random() < 0.8 else f"{random.randint(1, 3)}/{random.randint(65, 75)}"
            confidence_score = random.uniform(10, 30) if "0/" not in detection_ratio else 95
            threat_type = None
            malware_family = None
        
        # Generate timestamps
        last_seen = datetime.now(timezone.utc) - timedelta(days=random.randint(1, 365))
        scan_date = datetime.now(timezone.utc) - timedelta(hours=random.randint(1, 48))
        
        return ThreatIntelligenceResult(
            source_name=source,
            is_malicious=is_malicious,
            detection_ratio=detection_ratio,
            threat_type=threat_type,
            malware_family=malware_family,
            confidence_score=confidence_score,
            last_seen=last_seen,
            scan_date=scan_date
        )
    
    def generate_malware_metadata(self, hash_value: str, malware_family: Optional[str]) -> Optional[MalwareMetadata]:
        """Generate malware metadata if hash is malicious"""
        
        if not malware_family:
            return None
        
        family_info = self.MALWARE_FAMILIES.get(malware_family, {})
        
        # Generate simulated metadata
        file_names = [
            f"{malware_family}.exe",
            f"update_{random.randint(1000, 9999)}.exe",
            f"setup_{malware_family}.exe",
            f"{hash_value[:8]}.bin"
        ]
        
        detection_names = [
            f"{source}:{malware_family.capitalize()}" 
            for source in random.sample(self.THREAT_SOURCES, random.randint(3, 6))
        ]
        
        yara_rules = [
            f"rule_{malware_family}",
            f"generic_{family_info.get('type', 'malware')}",
            "suspicious_pe_header"
        ]
        
        behavioral_analysis = {
            "network_activity": random.choice([True, False]),
            "file_modifications": random.randint(5, 50),
            "registry_modifications": random.randint(10, 100),
            "processes_created": random.randint(1, 10),
            "mutex_created": random.choice([True, False]),
            "persistence_mechanisms": random.randint(1, 5)
        }
        
        return MalwareMetadata(
            file_names=file_names,
            file_size=random.randint(50000, 5000000),
            file_type="PE32 executable",
            first_seen=datetime.now(timezone.utc) - timedelta(days=random.randint(30, 365)),
            last_seen=datetime.now(timezone.utc) - timedelta(days=random.randint(1, 30)),
            submission_count=random.randint(10, 1000),
            detection_names=detection_names,
            yara_rules=yara_rules,
            behavioral_analysis=behavioral_analysis
        )
    
    def analyze_reputation(self, threat_results: List[ThreatIntelligenceResult]) -> ReputationAnalysis:
        """Analyze file reputation based on threat intelligence results"""
        
        total_vendors = len(threat_results)
        vendor_detections = sum(1 for result in threat_results if result.is_malicious)
        
        # Calculate trust score
        if vendor_detections == 0:
            trust_score = 95.0
        elif vendor_detections <= total_vendors * 0.1:  # Less than 10% detection
            trust_score = 70.0
        elif vendor_detections <= total_vendors * 0.3:  # Less than 30% detection
            trust_score = 40.0
        elif vendor_detections <= total_vendors * 0.6:  # Less than 60% detection
            trust_score = 20.0
        else:  # High detection rate
            trust_score = 5.0
        
        # Determine reputation level
        if trust_score >= 80:
            reputation_level = "trusted"
        elif trust_score >= 60:
            reputation_level = "neutral"
        elif trust_score >= 30:
            reputation_level = "suspicious"
        else:
            reputation_level = "malicious"
        
        # Simulate community votes
        if reputation_level == "malicious":
            community_votes = {"malicious": random.randint(50, 200), "harmless": random.randint(0, 10)}
        elif reputation_level == "suspicious":
            community_votes = {"malicious": random.randint(10, 50), "harmless": random.randint(10, 30)}
        else:
            community_votes = {"malicious": random.randint(0, 5), "harmless": random.randint(30, 100)}
        
        return ReputationAnalysis(
            trust_score=trust_score,
            reputation_level=reputation_level,
            vendor_detections=vendor_detections,
            total_vendors=total_vendors,
            community_votes=community_votes
        )
    
    def calculate_overall_verdict(self, reputation: ReputationAnalysis, 
                                 threat_results: List[ThreatIntelligenceResult]) -> Tuple[str, float]:
        """Calculate overall verdict and risk score"""
        
        # Count malicious detections
        malicious_count = sum(1 for result in threat_results if result.is_malicious)
        total_sources = len(threat_results)
        
        # Calculate risk score
        risk_score = 0
        
        if malicious_count > 0:
            detection_rate = malicious_count / total_sources
            risk_score = detection_rate * 100
            
            # Boost score based on confidence
            avg_confidence = sum(result.confidence_score for result in threat_results 
                               if result.is_malicious) / malicious_count
            risk_score = (risk_score + avg_confidence) / 2
        
        # Determine verdict
        if risk_score >= 70:
            verdict = "malicious"
        elif risk_score >= 30:
            verdict = "suspicious"
        else:
            verdict = "clean"
        
        return verdict, risk_score
    
    def generate_recommendations(self, verdict: str, risk_score: float, 
                               reputation: ReputationAnalysis) -> List[str]:
        """Generate security recommendations"""
        recommendations = []
        
        if verdict == "malicious":
            recommendations.extend([
                "URGENT: This file is identified as malware - DO NOT EXECUTE",
                "Quarantine or delete the file immediately",
                "Scan your system for additional malware",
                "Check if this file has already been executed on your system",
                "Report the hash to your security team",
                "Consider incident response procedures"
            ])
        elif verdict == "suspicious":
            recommendations.extend([
                "Exercise caution - file has suspicious characteristics",
                "Do not execute without thorough analysis",
                "Consider dynamic analysis in a sandbox environment",
                "Verify file source and legitimacy",
                "Monitor system if file was already executed"
            ])
        else:
            recommendations.extend([
                "File appears clean but continue monitoring",
                "Verify file source if obtained from untrusted location",
                "Keep antivirus definitions updated"
            ])
        
        # General recommendations
        recommendations.extend([
            "Maintain updated antivirus/anti-malware software",
            "Use application whitelisting where possible",
            "Implement defense-in-depth security measures",
            "Regular security awareness training for users"
        ])
        
        return recommendations
    
    async def check_hash(self, hash_value: str, check_multiple_sources: bool = True,
                        include_metadata: bool = True) -> Dict[str, Any]:
        """Perform comprehensive hash analysis"""
        
        # Validate hash
        hash_validation = self.validate_hash(hash_value)
        if not hash_validation.is_valid_hash:
            raise ValueError("Invalid hash format")
        
        # Check against threat intelligence sources
        threat_results = []
        sources_to_check = self.THREAT_SOURCES if check_multiple_sources else self.THREAT_SOURCES[:3]
        
        for source in sources_to_check:
            result = self.simulate_threat_intelligence_check(hash_value, source)
            threat_results.append(result)
        
        # Check for known malicious samples
        malware_family = self.check_known_malicious(hash_value)
        
        # Generate metadata if malicious
        malware_metadata = None
        if include_metadata and (malware_family or any(r.is_malicious for r in threat_results)):
            detected_family = malware_family or next(
                (r.malware_family for r in threat_results if r.malware_family), None
            )
            malware_metadata = self.generate_malware_metadata(hash_value, detected_family)
        
        # Analyze reputation
        reputation = self.analyze_reputation(threat_results)
        
        # Calculate overall verdict
        verdict, risk_score = self.calculate_overall_verdict(reputation, threat_results)
        
        # Generate recommendations
        recommendations = self.generate_recommendations(verdict, risk_score, reputation)
        
        return {
            'hash_validation': hash_validation,
            'threat_results': threat_results,
            'malware_metadata': malware_metadata,
            'reputation': reputation,
            'verdict': verdict,
            'risk_score': risk_score,
            'recommendations': recommendations
        }


async def execute_tool(input_data: MalwareHashInput) -> MalwareHashOutput:
    """Execute the malware hash checker tool"""
    
    try:
        checker = MalwareHashChecker()
        
        # Perform hash analysis
        results = await checker.check_hash(
            input_data.hash_value,
            input_data.check_multiple_sources,
            input_data.include_metadata
        )
        
        return MalwareHashOutput(
            success=True,
            hash_value=input_data.hash_value,
            hash_validation=results['hash_validation'],
            threat_intelligence=results['threat_results'],
            malware_metadata=results['malware_metadata'],
            reputation_analysis=results['reputation'],
            overall_verdict=results['verdict'],
            risk_score=results['risk_score'],
            recommendations=results['recommendations'],
            analysis_timestamp=datetime.now(timezone.utc)
        )
        
    except Exception as e:
        return MalwareHashOutput(
            success=False,
            hash_value=input_data.hash_value,
            hash_validation=HashValidation(
                is_valid_hash=False,
                detected_hash_type="unknown",
                hash_length=len(input_data.hash_value),
                hash_format_issues=[str(e)]
            ),
            threat_intelligence=[],
            malware_metadata=None,
            reputation_analysis=ReputationAnalysis(
                trust_score=0.0,
                reputation_level="unknown",
                vendor_detections=0,
                total_vendors=0,
                community_votes={}
            ),
            overall_verdict="unknown",
            risk_score=0.0,
            recommendations=[],
            analysis_timestamp=datetime.now(timezone.utc),
            error=str(e)
        )


# Tool metadata
TOOL_INFO = {
    "name": "malware_hash_checker",
    "display_name": "Malware Hash Checker",
    "description": "Check file hashes against threat intelligence sources and malware databases",
    "version": "1.0.0",
    "author": "Wildbox Security",
    "category": "malware_analysis"
}


# For testing
if __name__ == "__main__":
    import asyncio
    
    async def test():
        # Test with known malicious hash
        test_input = MalwareHashInput(
            hash_value="84c82835a5d21bbcf75a61706d8ab549",  # WannaCry sample
            check_multiple_sources=True,
            include_metadata=True
        )
        
        result = await execute_tool(test_input)
        print(f"Hash Check Success: {result.success}")
        print(f"Hash: {result.hash_value}")
        print(f"Hash Type: {result.hash_validation.detected_hash_type}")
        print(f"Overall Verdict: {result.overall_verdict}")
        print(f"Risk Score: {result.risk_score}")
        print(f"Vendor Detections: {result.reputation_analysis.vendor_detections}/{result.reputation_analysis.total_vendors}")
        
        if result.malware_metadata:
            print(f"Malware Family: {result.threat_intelligence[0].malware_family if result.threat_intelligence else 'Unknown'}")
    
    asyncio.run(test())
